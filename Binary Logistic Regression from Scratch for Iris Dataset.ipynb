{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1b0d06",
   "metadata": {},
   "source": [
    "# Welcome to the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ae8df",
   "metadata": {
    "id": "576ae8df"
   },
   "source": [
    "In this notebook, we will be creating binary logistic regression model from scratch and then use Iris dataset to test and train the model using one vs all method for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "be90dcac",
   "metadata": {
    "id": "be90dcac"
   },
   "outputs": [],
   "source": [
    "# importing necessary library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1c841c9f",
   "metadata": {
    "id": "1c841c9f"
   },
   "outputs": [],
   "source": [
    "# defining a linear regression class\n",
    "class Binary_Logistic_Regression:\n",
    "    def __init__(self, X, Y): # it takes X and Y meaning whole dataset and store it\n",
    "        self.X = np.vstack((X.values.T,np.ones(X.shape[0]))).T # adding an extra column so that we can multiply as this column is for theata zero.\n",
    "        self.Y = Y.reshape(-1,1) # Reshaping Y to the 2D array which have a single column\n",
    "        self.theta = np.random.random((1,self.X.shape[1])).T # Initializing values for theta's randomly for first time\n",
    "    def hypothesis(self): # creating a function which will return hypothesis\n",
    "        h = np.dot(self.X,self.theta)\n",
    "       # print(\"h: \",h)\n",
    "        h = self.sigmoid(h)\n",
    "        #print(h)\n",
    "        return h\n",
    "\n",
    "    def cost_function(self): # function to calculate cost of the model\n",
    "        h = self.hypothesis()\n",
    "        a = np.mean(-((self.Y * np.log(h)) + ((1-self.Y) * np.log(1-h))))\n",
    "        #print(a)\n",
    "        return a\n",
    "\n",
    "    def sigmoid(self,h):\n",
    "        return (1/(1+np.exp(-h)))\n",
    "\n",
    "    def der_cost_function(self): #function for derivative of cost function as it is used in updating the weights\n",
    "        a = self.hypothesis() - self.Y \n",
    "        x = len(self.Y)\n",
    "        var = np.dot(self.X.T , (a)) / x\n",
    "        return var\n",
    "\n",
    "    def gradient_descent_function(self,lr=0.0001): # function for calculating gradient descent\n",
    "        der = self.der_cost_function()\n",
    "       # print(\"der:\",der)\n",
    "        return self.theta-lr*der\n",
    "\n",
    "    def train_function(self,no_of_it_train,no_of_it_print_cost,lr=0.0001): # Function to train the model\n",
    "        count=1 # for printing cost\n",
    "        \n",
    "        for i in range(1,no_of_it_train+1):\n",
    "            gradient_val = self.gradient_descent_function(lr) # finding new weights\n",
    "            #print(gradient_val)\n",
    "            self.theta = gradient_val # updating the weights\n",
    "            if(count == no_of_it_print_cost):\n",
    "                print(f\"Cost:{self.cost_function()} \") # printing cost value after some number of iterations\n",
    "                count=1\n",
    "            count+=1\n",
    "    def Predict(self,X): # It is used for prediction\n",
    "        predictions= np.dot(X,self.theta)\n",
    "        return self.sigmoid(predictions)\n",
    "    def get_weights(self): # Function for getting the weights\n",
    "        return self.theta\n",
    "    def printer(self): # Function to print X, Y, theta's\n",
    "        print(\"X: \",self.X)\n",
    "        print(\"Y: \",self.Y)\n",
    "        print(\"theta: \",self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "eaf96af2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "eaf96af2",
    "outputId": "289c56b4-43f9-46b1-b1d1-eb91c2c9a266",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width         species\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"iris.csv\") # using Iris data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8e1a701e",
   "metadata": {
    "id": "8e1a701e"
   },
   "outputs": [],
   "source": [
    "# splitting the dataset and converting non-numarical column to numarical using LabelEncoder()\n",
    "x=df.drop('species',axis=1)\n",
    "y=df['species']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "# declaring label binarizer for each class\n",
    "label_binarizer_setosa = LabelBinarizer()\n",
    "label_binarizer_versicolor = LabelBinarizer()\n",
    "label_binarizer_virginica = LabelBinarizer()\n",
    "\n",
    "# converting for each class\n",
    "setosa_vs_all = label_binarizer_setosa.fit_transform(y == 0)\n",
    "versicolor_vs_all = label_binarizer_versicolor.fit_transform(y == 1)\n",
    "virginica_vs_all = label_binarizer_virginica.fit_transform(y == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "03a6f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different splits of dataset\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(x, setosa_vs_all, test_size=0.20, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x, versicolor_vs_all, test_size=0.20, random_state=42)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(x, virginica_vs_all, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6a1779ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a1779ca",
    "outputId": "4df0a6e3-9cd2-48a8-e2cf-79f7a2a015ff"
   },
   "outputs": [],
   "source": [
    "# Initializing our model\n",
    "model1=Binary_Logistic_Regression(X_train1,y_train1)\n",
    "model2=Binary_Logistic_Regression(X_train2,y_train2)\n",
    "model3=Binary_Logistic_Regression(X_train3,y_train3)\n",
    "#lr.printer() #checking if everthing is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "22e58816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22e58816",
    "outputId": "7b969011-24c7-4d3b-f930-75707a7fef96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------Model 1---------------------------------\n",
      "Cost:3.6564078568324265 \n",
      "Cost:0.9318263019313628 \n",
      "Cost:0.3900107228077719 \n",
      "Cost:0.3380365793624955 \n",
      "Cost:0.3187597536905528 \n",
      "Cost:0.3039208670288789 \n",
      "Cost:0.2906061682656864 \n",
      "Cost:0.2783522107702429 \n",
      "Cost:0.2670119126754716 \n",
      "Cost:0.25649130246080537 \n",
      "Cost:0.2467120476946595 \n",
      "Cost:0.2376050974558442 \n",
      "Cost:0.2291089576876154 \n",
      "Cost:0.2211687363918994 \n",
      "Cost:0.21373537491622185 \n",
      "Cost:0.20676497424709334 \n",
      "Cost:0.20021819861292953 \n",
      "Cost:0.19405974859278755 \n",
      "Cost:0.18825789688655362 \n",
      "Cost:0.18278408004621133 \n",
      "Cost:0.1776125397861337 \n",
      "Cost:0.17272000797356502 \n",
      "Cost:0.16808542995786427 \n",
      "Cost:0.16368972146695124 \n",
      "Cost:0.15951555484748178 \n",
      "Cost:0.1555471709341952 \n",
      "Cost:0.1517702132961118 \n",
      "Cost:0.14817158202089942 \n",
      "Cost:0.14473930456509773 \n",
      "Cost:0.14146242151994132 \n",
      "Cost:0.13833088542415548 \n",
      "Cost:0.13533547100037857 \n",
      "Cost:0.13246769540492956 \n",
      "Cost:0.12971974726533628 \n",
      "Cost:0.1270844234399727 \n",
      "Cost:0.12455507257250004 \n",
      "Cost:0.12212554463349669 \n",
      "Cost:0.11979014574516288 \n",
      "Cost:0.11754359767453823 \n",
      "Cost:0.11538100145820158 \n",
      "Cost:0.11329780468856661 \n",
      "Cost:0.11128977205010333 \n",
      "Cost:0.10935295874434008 \n",
      "Cost:0.10748368648638461 \n",
      "Cost:0.10567852179386185 \n",
      "Cost:0.10393425632241264 \n",
      "Cost:0.10224788903084535 \n",
      "Cost:0.10061660998434079 \n",
      "Cost:0.0990377856261961 \n",
      "Cost:0.09750894536794949 \n",
      "Cost:0.09602776936466172 \n",
      "Cost:0.09459207735702085 \n",
      "Cost:0.09319981847499687 \n",
      "Cost:0.09184906190927403 \n",
      "Cost:0.09053798836682755 \n",
      "Cost:0.08926488223593859 \n",
      "Cost:0.08802812439385127 \n",
      "Cost:0.08682618559725166 \n",
      "Cost:0.0856576204019436 \n",
      "Cost:0.0845210615635765 \n",
      "Cost:0.08341521487615818 \n",
      "Cost:0.08233885440942133 \n",
      "Cost:0.08129081810996208 \n",
      "Cost:0.08027000373451547 \n",
      "Cost:0.07927536508679583 \n",
      "Cost:0.07830590853207905 \n",
      "Cost:0.07736068976614614 \n",
      "Cost:0.07643881081741886 \n",
      "Cost:0.07553941726307165 \n",
      "Cost:0.07466169564169058 \n",
      "Cost:0.07380487104662206 \n",
      "Cost:0.07296820488559837 \n",
      "Cost:0.0721509927935049 \n",
      "Cost:0.07135256268632013 \n",
      "Cost:0.07057227294530705 \n",
      "Cost:0.06980951072147817 \n",
      "Cost:0.0690636903512186 \n",
      "Cost:0.06833425187472017 \n",
      "Cost:0.06762065964958562 \n",
      "Cost:0.06692240105260111 \n",
      "Cost:0.06623898526324858 \n",
      "Cost:0.06556994212305917 \n",
      "Cost:0.06491482106538456 \n",
      "Cost:0.06427319011060356 \n",
      "Cost:0.06364463492216811 \n",
      "Cost:0.06302875791926786 \n",
      "Cost:0.062425177442208206 \n",
      "Cost:0.06183352696691377 \n",
      "Cost:0.06125345436522938 \n",
      "Cost:0.060684621207956375 \n",
      "Cost:0.060126702107785354 \n",
      "Cost:0.05957938409950108 \n",
      "Cost:0.059042366055031374 \n",
      "Cost:0.058515358131082544 \n",
      "Cost:0.05799808124728057 \n",
      "Cost:0.057490266592875754 \n",
      "Cost:0.0569916551602134 \n",
      "Cost:0.05650199730330089 \n",
      "Cost:0.05602105231991466 \n",
      "Cost:0.055548588055805995 \n",
      "---------------------------------Model 2---------------------------------\n",
      "Cost:2.5741827857301636 \n",
      "Cost:0.8992853718849589 \n",
      "Cost:0.6658706960725269 \n",
      "Cost:0.6473166815869097 \n",
      "Cost:0.6402945354147764 \n",
      "Cost:0.634490786848332 \n",
      "Cost:0.6292476612207781 \n",
      "Cost:0.624476297661684 \n",
      "Cost:0.6201280549942925 \n",
      "Cost:0.6161610851498743 \n",
      "Cost:0.6125378777659309 \n",
      "Cost:0.6092247495703338 \n",
      "Cost:0.6061914686657658 \n",
      "Cost:0.6034109101287143 \n",
      "Cost:0.6008587393354745 \n",
      "Cost:0.5985131230444717 \n",
      "Cost:0.596354467416246 \n",
      "Cost:0.5943651815747845 \n",
      "Cost:0.5925294650052282 \n",
      "Cost:0.590833116938624 \n",
      "Cost:0.589263365836612 \n",
      "Cost:0.5878087171219436 \n",
      "Cost:0.586458817379657 \n",
      "Cost:0.5852043333607746 \n",
      "Cost:0.5840368442427291 \n",
      "Cost:0.5829487457293306 \n",
      "Cost:0.5819331647017992 \n",
      "Cost:0.5809838832570206 \n",
      "Cost:0.5800952710872266 \n",
      "Cost:0.5792622252652061 \n",
      "Cost:0.5784801166002296 \n",
      "Cost:0.5777447418219371 \n",
      "Cost:0.5770522809326799 \n",
      "Cost:0.5763992591435877 \n",
      "Cost:0.5757825128765617 \n",
      "Cost:0.5751991593740056 \n",
      "Cost:0.5746465695110978 \n",
      "Cost:0.5741223434523747 \n",
      "Cost:0.5736242888359613 \n",
      "Cost:0.5731504012055103 \n",
      "Cost:0.5726988464423395 \n",
      "Cost:0.5722679449788566 \n",
      "Cost:0.5718561575995795 \n",
      "Cost:0.5714620726582893 \n",
      "Cost:0.5710843945594457 \n",
      "Cost:0.570721933369263 \n",
      "Cost:0.5703735954370692 \n",
      "Cost:0.5700383749209947 \n",
      "Cost:0.569715346123885 \n",
      "Cost:0.5694036565557787 \n",
      "Cost:0.5691025206485318 \n",
      "Cost:0.5688112140563173 \n",
      "Cost:0.5685290684829475 \n",
      "Cost:0.5682554669833512 \n",
      "Cost:0.567989839692176 \n",
      "Cost:0.56773165993751 \n",
      "Cost:0.5674804407021435 \n",
      "Cost:0.5672357313987424 \n",
      "Cost:0.5669971149287998 \n",
      "Cost:0.5667642049983506 \n",
      "Cost:0.5665366436661958 \n",
      "Cost:0.5663140991028589 \n",
      "Cost:0.5660962635406845 \n",
      "Cost:0.5658828513974634 \n",
      "Cost:0.565673597557703 \n",
      "Cost:0.5654682557972445 \n",
      "Cost:0.5652665973383117 \n",
      "Cost:0.5650684095233418 \n",
      "Cost:0.5648734945970609 \n",
      "Cost:0.564681668587284 \n",
      "Cost:0.5644927602758102 \n",
      "Cost:0.564306610251609 \n",
      "Cost:0.5641230700392091 \n",
      "Cost:0.5639420012958716 \n",
      "Cost:0.5637632750717082 \n",
      "Cost:0.5635867711274456 \n",
      "Cost:0.563412377305015 \n",
      "Cost:0.5632399889465792 \n",
      "Cost:0.5630695083580027 \n",
      "Cost:0.5629008443131222 \n",
      "Cost:0.5627339115954999 \n",
      "Cost:0.562568630574626 \n",
      "Cost:0.5624049268138053 \n",
      "Cost:0.5622427307071998 \n",
      "Cost:0.5620819771437124 \n",
      "Cost:0.5619226051955974 \n",
      "Cost:0.5617645578298627 \n",
      "Cost:0.5616077816406851 \n",
      "Cost:0.5614522266012152 \n",
      "Cost:0.5612978458332784 \n",
      "Cost:0.5611445953936023 \n",
      "Cost:0.5609924340753148 \n",
      "Cost:0.5608413232235556 \n",
      "Cost:0.5606912265641384 \n",
      "Cost:0.5605421100442911 \n",
      "Cost:0.5603939416845688 \n",
      "Cost:0.5602466914411172 \n",
      "Cost:0.5601003310775187 \n",
      "Cost:0.5599548340455246 \n",
      "Cost:0.5598101753740184 \n",
      "---------------------------------Model 3---------------------------------\n",
      "Cost:1.386824861781508 \n",
      "Cost:0.7282108388556544 \n",
      "Cost:0.6616142398921896 \n",
      "Cost:0.6351847463434216 \n",
      "Cost:0.6129539638123092 \n",
      "Cost:0.5929075095578963 \n",
      "Cost:0.574722803820219 \n",
      "Cost:0.5581878177541684 \n",
      "Cost:0.5431175168633475 \n",
      "Cost:0.5293480939721333 \n",
      "Cost:0.5167349482706564 \n",
      "Cost:0.505150757204584 \n",
      "Cost:0.4944835855173927 \n",
      "Cost:0.48463509824680234 \n",
      "Cost:0.47551891601502755 \n",
      "Cost:0.46705912885456596 \n",
      "Cost:0.45918897091584654 \n",
      "Cost:0.4518496499829572 \n",
      "Cost:0.44498932097176186 \n",
      "Cost:0.4385621902725125 \n",
      "Cost:0.432527737064058 \n",
      "Cost:0.42685003796742343 \n",
      "Cost:0.4214971822118352 \n",
      "Cost:0.41644076558285736 \n",
      "Cost:0.41165545263383246 \n",
      "Cost:0.4071185978591683 \n",
      "Cost:0.4028099176872253 \n",
      "Cost:0.39871120621766204 \n",
      "Cost:0.39480608858817756 \n",
      "Cost:0.39107980670565873 \n",
      "Cost:0.38751903282077405 \n",
      "Cost:0.38411170707085357 \n",
      "Cost:0.38084689567295255 \n",
      "Cost:0.3777146669274444 \n",
      "Cost:0.3747059826020274 \n",
      "Cost:0.3718126026158986 \n",
      "Cost:0.3690270012422295 \n",
      "Cost:0.3663422933014123 \n",
      "Cost:0.36375216903421453 \n",
      "Cost:0.3612508365285804 \n",
      "Cost:0.3588329707311763 \n",
      "Cost:0.35649366820896855 \n",
      "Cost:0.3542284069406689 \n",
      "Cost:0.35203301051576125 \n",
      "Cost:0.34990361620253957 \n",
      "Cost:0.3478366464182931 \n",
      "Cost:0.3458287831962764 \n",
      "Cost:0.34387694529690715 \n",
      "Cost:0.34197826765606654 \n",
      "Cost:0.3401300829025008 \n",
      "Cost:0.338329904710082 \n",
      "Cost:0.33657541277984643 \n",
      "Cost:0.33486443927197673 \n",
      "Cost:0.333194956529763 \n",
      "Cost:0.33156506595658236 \n",
      "Cost:0.3299729879234391 \n",
      "Cost:0.32841705259900233 \n",
      "Cost:0.32689569160661247 \n",
      "Cost:0.3254074304236968 \n",
      "Cost:0.32395088144862 \n",
      "Cost:0.32252473766840134 \n",
      "Cost:0.32112776686810207 \n",
      "Cost:0.31975880632917547 \n",
      "Cost:0.3184167579697719 \n",
      "Cost:0.31710058388502854 \n",
      "Cost:0.31580930224980863 \n",
      "Cost:0.3145419835502846 \n",
      "Cost:0.3132977471142387 \n",
      "Cost:0.31207575791302566 \n",
      "Cost:0.3108752236108918 \n",
      "Cost:0.3096953918397632 \n",
      "Cost:0.3085355476797893 \n",
      "Cost:0.3073950113278565 \n",
      "Cost:0.3062731359379986 \n",
      "Cost:0.3051693056191847 \n",
      "Cost:0.30408293357732225 \n",
      "Cost:0.3030134603895632 \n",
      "Cost:0.30196035240009633 \n",
      "Cost:0.30092310022759816 \n",
      "Cost:0.2999012173754166 \n",
      "Cost:0.29889423893635647 \n",
      "Cost:0.29790172038466006 \n",
      "Cost:0.29692323644842855 \n",
      "Cost:0.29595838005631736 \n",
      "Cost:0.29500676135287424 \n",
      "Cost:0.2940680067773628 \n",
      "Cost:0.29314175820135496 \n",
      "Cost:0.2922276721207708 \n",
      "Cost:0.29132541889839914 \n",
      "Cost:0.2904346820532619 \n",
      "Cost:0.28955515759347544 \n",
      "Cost:0.2886865533895363 \n",
      "Cost:0.28782858858520394 \n",
      "Cost:0.28698099304337216 \n",
      "Cost:0.2861435068245252 \n",
      "Cost:0.28531587969557015 \n",
      "Cost:0.284497870666992 \n",
      "Cost:0.2836892475564467 \n",
      "Cost:0.28288978657704245 \n",
      "Cost:0.2820992719486936 \n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------Model 1---------------------------------\")\n",
    "model1.train_function(100000,1000) # training our model 1 for 10000 epochs and print cost after every 100th epoch\n",
    "print(\"---------------------------------Model 2---------------------------------\")\n",
    "model2.train_function(100000,1000) # training our model 2 for 10000 epochs and print cost after every 100th epoch\n",
    "print(\"---------------------------------Model 3---------------------------------\")\n",
    "model3.train_function(100000,1000) # training our model 3 for 10000 epochs and print cost after every 100th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ade78c7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ade78c7f",
    "outputId": "7e712826-7c77-4253-c0ef-15b787b3c19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------Model 1---------------------------------\n",
      "[[2.95963018e-02]\n",
      " [9.27175998e-01]\n",
      " [6.49377166e-04]\n",
      " [3.85809657e-02]\n",
      " [2.49864674e-02]\n",
      " [9.06882605e-01]\n",
      " [1.38120818e-01]\n",
      " [1.57431102e-02]\n",
      " [1.70030182e-02]\n",
      " [7.89233883e-02]\n",
      " [1.95683829e-02]\n",
      " [8.82159416e-01]\n",
      " [9.42349803e-01]\n",
      " [8.80492247e-01]\n",
      " [9.41866487e-01]\n",
      " [4.57484639e-02]\n",
      " [5.04087022e-03]\n",
      " [6.44069863e-02]\n",
      " [3.64104153e-02]\n",
      " [5.25078899e-03]\n",
      " [8.70877203e-01]\n",
      " [2.16599082e-02]\n",
      " [8.90096625e-01]\n",
      " [5.49131804e-03]\n",
      " [6.88163066e-03]\n",
      " [1.18386172e-02]\n",
      " [3.33183212e-03]\n",
      " [5.45907269e-03]\n",
      " [8.72469983e-01]\n",
      " [8.57543513e-01]]\n",
      "---------------------------------Model 2---------------------------------\n",
      "[[0.41691185]\n",
      " [0.13268689]\n",
      " [0.61837728]\n",
      " [0.38720926]\n",
      " [0.44915968]\n",
      " [0.17636472]\n",
      " [0.34138906]\n",
      " [0.40373228]\n",
      " [0.56823221]\n",
      " [0.40347803]\n",
      " [0.36127761]\n",
      " [0.22062768]\n",
      " [0.1587169 ]\n",
      " [0.20854115]\n",
      " [0.11976243]\n",
      " [0.31385601]\n",
      " [0.4358251 ]\n",
      " [0.44367443]\n",
      " [0.39738872]\n",
      " [0.47508074]\n",
      " [0.19098591]\n",
      " [0.38477849]\n",
      " [0.16982427]\n",
      " [0.47344268]\n",
      " [0.31356443]\n",
      " [0.42426942]\n",
      " [0.56173026]\n",
      " [0.40274798]\n",
      " [0.22289529]\n",
      " [0.20946459]]\n",
      "---------------------------------Model 3---------------------------------\n",
      "[[0.40766041]\n",
      " [0.0083313 ]\n",
      " [0.85199452]\n",
      " [0.41524957]\n",
      " [0.31854148]\n",
      " [0.01078364]\n",
      " [0.22550538]\n",
      " [0.51897277]\n",
      " [0.43400587]\n",
      " [0.2596021 ]\n",
      " [0.54776215]\n",
      " [0.01553077]\n",
      " [0.0063738 ]\n",
      " [0.01541117]\n",
      " [0.01140619]\n",
      " [0.38850039]\n",
      " [0.78373935]\n",
      " [0.29774064]\n",
      " [0.4574751 ]\n",
      " [0.77011297]\n",
      " [0.02172437]\n",
      " [0.56140328]\n",
      " [0.01768708]\n",
      " [0.75664325]\n",
      " [0.56480427]\n",
      " [0.60514891]\n",
      " [0.72912327]\n",
      " [0.75541553]\n",
      " [0.01798431]\n",
      " [0.02055707]]\n"
     ]
    }
   ],
   "source": [
    "# testing our model\n",
    "X_test1 = np.vstack((X_test1.values.T,np.ones(X_test1.shape[0]))).T# adding an extra column so that we can multiply as this column is for theata zero.\n",
    "X_test2 = np.vstack((X_test2.values.T,np.ones(X_test2.shape[0]))).T# adding an extra column so that we can multiply as this column is for theata zero.\n",
    "X_test3 = np.vstack((X_test3.values.T,np.ones(X_test3.shape[0]))).T# adding an extra column so that we can multiply as this column is for theata zero.\n",
    "print(\"---------------------------------Model 1---------------------------------\")\n",
    "models_predictions = []\n",
    "print(model1.Predict(X_test1))\n",
    "models_predictions.append(model1.Predict(X_test1))\n",
    "print(\"---------------------------------Model 2---------------------------------\")\n",
    "print(model2.Predict(X_test2))\n",
    "models_predictions.append(model2.Predict(X_test2))\n",
    "print(\"---------------------------------Model 3---------------------------------\")\n",
    "print(model3.Predict(X_test3))\n",
    "models_predictions.append(model3.Predict(X_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7FRoSjxMxBWn",
   "metadata": {
    "id": "7FRoSjxMxBWn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediciting confidence\n",
    "def predict_confidence(models_predictions):\n",
    "    final_predictions=[]\n",
    "    for i in range(len(models_predictions[1])):\n",
    "        final_predictions.append(np.argmax([scores[i] for scores in models_predictions]))\n",
    "    return final_predictions\n",
    "predict_confidence(models_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "73da0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def accuracy(model_predictions, y):\n",
    "        mse = np.mean((model_predictions - y) ** 2)\n",
    "        acc = 1 - mse / np.var(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c6d1374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy:\n",
      " 0.9772423218137708 \n",
      "\n",
      "Model 2 Accuracy:\n",
      " 0.09563347322713178 \n",
      "\n",
      "Model 3 Accuracy:\n",
      " 0.6432378415357807 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy for these three models\n",
    "print(\"Model 1 Accuracy:\\n\",accuracy(models_predictions[0], y_test1),\"\\n\")\n",
    "print(\"Model 2 Accuracy:\\n\",accuracy(models_predictions[1], y_test2),\"\\n\")\n",
    "print(\"Model 3 Accuracy:\\n\",accuracy(models_predictions[2], y_test3),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8242d",
   "metadata": {},
   "source": [
    "Happy Coding :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
